import pandas as pd

df = pd.read_csv(r'IMDB Dataset.csv')
print(df)
print(df.sentiment.value_counts())
print(df.isnull().sum())

#Plotting no.of reviews in each statement
import seaborn as sn
import matplotlib.pyplot as plt

sn.set_style('darkgrid')
plt.figure(figsize=(4,5))
sn.countplot(df.sentiment)
plt.title("Count of reviews")
plt.xlabel('Review')
plt.ylabel('Count')
print(plt.show())

sample_data=df.review.loc[1]
print(sample_data)

#Removing html Tags
import re
from bs4 import BeautifulSoup

soup=BeautifulSoup(sample_data,'html.parser')
sample_data=soup.get_text()
print(sample_data)

#Removing symbols with Regular Expressions
sample_data=re.sub('[^a-zA-Z]',' ',sample_data)
print(sample_data)

#Covert to lower case
print(sample_data.lower())

#Removing stop words which can be done after splitting each word into a list
sample_data=sample_data.split()
print(sample_data)

import nltk
stop_word=nltk.corpus.stopwords.words('english')
print(stop_word[0:10])

#Removing stop words before removing punctuations and symbols
import csv
sentence=[]
label=[]
with open(r'C:\Users\abhishek.sakthivel\Downloads\archive (1)\IMDB Dataset.csv','r',encoding='utf-8') as csvfile:
    read=csv.reader(csvfile,delimiter=',')
    next(read)
    for row in read:
        label.append(row[1])
        sentence_1=row[0]
        for word in stop_word:
            token=' '+word+' '
            sentence_1=sentence_1.replace(token,' ')
        sentence.append(sentence_1)
#print(sentence)
#print(label[0])

#Encoding sentiments with 1,0
from sklearn import preprocessing
label_encoder=preprocessing.LabelEncoder()
labels=label_encoder.fit_transform(label)
print(labels[0:10])

#Splitting the data
training_portion=0.8 #Taking 80% data for training
train_size=int(len(sentence)* training_portion)

train_sentence=sentence[:train_size]
train_label=labels[:train_size]

validation_sentence=sentence[train_size:]
validation_label=labels[train_size:]

print(len(train_sentence))
print(len(train_label))
print(len(validation_sentence))
print(len(validation_label))

import numpy as np
training_label_final=np.array(train_label)
testing_label_final = np.array(validation_label)

#Tokenizing
vocab_size=10000
embedding_dim=32
max_length=120
trunc_type='post'
oov_tok='<OOV>'

from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences

tokenizers=Tokenizer(num_words=vocab_size,oov_token=oov_tok)
tokenizers.fit_on_texts(train_sentence)
word_index=tokenizers.word_index
sequence=tokenizers.texts_to_sequences(train_sentence)
padded=pad_sequences(sequence,maxlen=max_length,truncating=trunc_type)
#We are taking maximum 120 words from a sentence, any sentence goes beyond that will be truncated till last.

testing_sequence=tokenizers.texts_to_sequences(validation_sentence)
testing_padded=pad_sequences(testing_sequence,maxlen=max_length)

import tensorflow as tf

model=tf.keras.Sequential([
    tf.keras.layers.Embedding(vocab_size,embedding_dim,input_length=max_length),
    tf.keras.layers.Bidirectional(tf.keras.layers.GRU(32)),
    tf.keras.layers.Dense(6,activation='relu'),
    tf.keras.layers.Dense(1,activation='sigmoid')
])

model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])
model.summary()

n_epochs=10
hist=model.fit(padded,training_label_final,epochs=n_epochs,validation_data=(testing_padded,
                                                                            testing_label_final))

def plot_graph(history,string):
    plt.plot(history.history[string])
    plt.plot(history.history['val_'+string])
    plt.xlabel('Epoch')
    plt.ylabel(string)
    plt.legend([string,'val_'+string])
    print(plt.show())


plot_graph(hist,'accuracy')
plot_graph(hist,'loss')

#Model with Conv-1D
model2=tf.keras.Sequential([
    tf.keras.layers.Embedding(vocab_size,embedding_dim,input_length=max_length),
    tf.keras.layers.Conv1D(128,5,activation='relu'), #Because it is a text- so 1D
    tf.keras.layers.GlobalAvgPool1D(),
    tf.keras.layers.Dense(6,activation='relu'),
    tf.keras.layers.Dense(1,activation='sigmoid')
])

model2.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])
model2.summary() #study the parameters and neurons in each layer for interview

n_epochs=10
hist=model2.fit(padded,training_label_final,epochs=n_epochs,validation_data=(testing_padded,
                                                                            testing_label_final))

plot_graph(hist,'accuracy')
plot_graph(hist,'loss')

#Model with LSTM
model3=tf.keras.Sequential([
    tf.keras.layers.Embedding(vocab_size,embedding_dim,input_length=max_length),
    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64)),
    tf.keras.layers.Dense(6,activation='relu'),
    tf.keras.layers.Dense(1,activation='sigmoid')
])

n_epochs=10
hist=model3.fit(padded,training_label_final,epochs=n_epochs,validation_data=(testing_padded,
                                                                            testing_label_final))

plot_graph(hist,'accuracy')
plot_graph(hist,'loss')

#With Multi-Layer LSTM model
model4=tf.keras.Sequential([
    tf.keras.layers.Embedding(vocab_size,embedding_dim,input_length=max_length),
    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64,return_sequences=True)),#To maintain the same input neurons as previous instead of having only this 128 - see it in summary
    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32)),
    tf.keras.layers.Dense(6,activation='relu'),
    tf.keras.layers.Dense(1,activation='sigmoid')
])

model4.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])
model4.summary()

n_epochs=10
hist=model4.fit(padded,training_label_final,epochs=n_epochs,validation_data=(testing_padded,
                                                                            testing_label_final))

#Combining all the models
model5=tf.keras.Sequential([
    tf.keras.layers.Embedding(vocab_size,embedding_dim,input_length=max_length),
    tf.keras.layers.Dropout(0.2),
    tf.keras.layers.Conv1D(128,5,activation='relu'),
    tf.keras.layers.MaxPooling1D(pool_size=4),
    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(128,return_sequences=True)),#To maintain the same input neurons as previous instead of having only this 128 - see it in summary
    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(80,return_sequences=True)),
    tf.keras.layers.MaxPooling1D(pool_size=2),
    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64,return_sequences=True)),
    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32)),
    tf.keras.layers.Dense(6,activation='relu'),
    tf.keras.layers.Dense(1,activation='sigmoid')])

model5.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])
model5.summary()

n_epochs=10
hist=model5.fit(padded,training_label_final,epochs=n_epochs,validation_data=(testing_padded,
                                                                            testing_label_final))

#Difference between Average pooling & Max-pooling, and how the neurons are distributed - research it
#Here maxpooling = 4, the neurons reudce by 4 times from 116 to 29, but the 128 remains same - look at summary
#In averagepooling - the 116 disappears and only 128 will be there

plot_graph(hist,'accuracy')
plot_graph(hist,'loss')


